{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7a1d31",
   "metadata": {},
   "source": [
    "# NexusFlow Complete Demo: Multi-Table ML with Advanced Features\n",
    "\n",
    "Welcome to NexusFlow! This notebook demonstrates how to use NexusFlow's advanced multi-table machine learning capabilities with synthetic data. You'll learn how to train sophisticated models that can learn from multiple related datasets simultaneously.\n",
    "\n",
    "## What is NexusFlow?\n",
    "\n",
    "NexusFlow is a cutting-edge machine learning framework designed for **multi-table relational learning**. Unlike traditional ML approaches that work with single flattened datasets, NexusFlow can:\n",
    "\n",
    "- üîó **Learn from multiple related tables simultaneously**\n",
    "- üß† **Use advanced transformer architectures** (FT-Transformer, TabNet, Standard)\n",
    "- ‚ö° **Leverage FlashAttention** for efficient processing\n",
    "- üîÄ **Apply Mixture of Experts (MoE)** for complex patterns\n",
    "- üìä **Advanced preprocessing pipelines** with automatic feature detection\n",
    "- üéØ **Cross-contextual attention** to capture relationships between tables\n",
    "- üì¶ **Model optimization** with quantization and pruning\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7a05c",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/ArkChaudhary/NexusFlow.git\n",
    "!pip install -r NexusFlow/requirements.txt\n",
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75635d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NexusFlow imports\n",
    "import sys\n",
    "sys.path.insert(0, './NexusFlow/src')\n",
    "from nexusflow.project_manager import ProjectManager\n",
    "from nexusflow.config import load_config_from_file\n",
    "from nexusflow.trainer.trainer import Trainer\n",
    "from nexusflow.api.model_api import load_model, extract_pytorch_model, NexusFlowModel, create_optimized_model\n",
    "from nexusflow.optimization.optimizer import optimize_model\n",
    "\n",
    "print(\"üöÄ NexusFlow environment ready!\")\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f06b76",
   "metadata": {},
   "source": [
    "## Section 2: Project Initialization & Data Preparation\n",
    "\n",
    "### Setting up the Project Structure\n",
    "\n",
    "NexusFlow follows a standardized project structure that keeps everything organized:\n",
    "\n",
    "```\n",
    "nexusflow_project/\n",
    "‚îú‚îÄ‚îÄ configs/          # Configuration files\n",
    "‚îú‚îÄ‚îÄ datasets/         # Raw data files\n",
    "‚îú‚îÄ‚îÄ models/          # Saved model artifacts\n",
    "‚îú‚îÄ‚îÄ results/         # Training results and logs\n",
    "‚îú‚îÄ‚îÄ notebooks/       # Jupyter notebooks\n",
    "‚îî‚îÄ‚îÄ src/            # Source code (if extending)\n",
    "```\n",
    "\n",
    "Let's create this structure and populate it with synthetic data for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize project structure\n",
    "pm = ProjectManager()\n",
    "project_path = pm.init_project('nexusflow_colab_demo', force=True)\n",
    "datasets_path = os.path.join(project_path, 'datasets')\n",
    "os.makedirs(datasets_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "%cd nexusflow_colab_demo\n",
    "print(f\"üìÅ Project created at: {os.getcwd()}\")\n",
    "print(\"üìÇ Project structure:\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965c3b",
   "metadata": {},
   "source": [
    "### Creating Synthetic Multi-Table Data\n",
    "\n",
    "For this tutorial, we'll create two related datasets:\n",
    "1. **`users.csv`** - User profile information (demographics, preferences)  \n",
    "2. **`transactions.csv`** - User transaction history (purchases, amounts, categories)\n",
    "\n",
    "Our goal will be to predict user **churn risk** based on both profile and transaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate Users Dataset\n",
    "n_users = 1000\n",
    "user_ids = range(1, n_users + 1)\n",
    "\n",
    "users_data = {\n",
    "    'user_id': user_ids,\n",
    "    'age': np.random.normal(35, 12, n_users).astype(int).clip(18, 80),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_users).astype(int).clip(20000, 200000),\n",
    "    'account_tenure_months': np.random.exponential(24, n_users).astype(int).clip(1, 120),\n",
    "    'num_products': np.random.poisson(2.5, n_users).clip(1, 8),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_users, \n",
    "                              p=[0.2, 0.25, 0.2, 0.15, 0.2]),\n",
    "    'subscription_tier': np.random.choice(['Basic', 'Premium', 'Enterprise'], n_users, \n",
    "                                        p=[0.5, 0.35, 0.15]),\n",
    "    'support_tickets': np.random.poisson(1.2, n_users),\n",
    "}\n",
    "\n",
    "users_df = pd.DataFrame(users_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0992d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Transactions Dataset\n",
    "n_transactions = 5000\n",
    "transaction_user_ids = np.random.choice(user_ids, n_transactions, \n",
    "                                      p=np.random.dirichlet(np.ones(n_users)))\n",
    "\n",
    "transactions_data = {\n",
    "    'user_id': transaction_user_ids,\n",
    "    'transaction_amount': np.random.lognormal(4, 1, n_transactions).clip(10, 5000),\n",
    "    'transaction_frequency': np.random.exponential(2, n_transactions).clip(0.1, 20),\n",
    "    'days_since_last_transaction': np.random.exponential(7, n_transactions).astype(int).clip(0, 365),\n",
    "    'payment_method': np.random.choice(['Credit', 'Debit', 'Digital', 'Cash'], n_transactions,\n",
    "                                     p=[0.4, 0.3, 0.25, 0.05]),\n",
    "    'transaction_category': np.random.choice(['Retail', 'Services', 'Entertainment', 'Food', 'Other'], \n",
    "                                           n_transactions, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "    'merchant_rating': np.random.beta(8, 2, n_transactions) * 5,  # Skewed toward high ratings\n",
    "}\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2908a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic churn labels with realistic patterns\n",
    "# Higher churn probability for: low income, few products, high support tickets, low transaction frequency\n",
    "churn_probability = (\n",
    "    0.1 +  # Base rate\n",
    "    0.15 * (users_df['income'] < 40000) +  # Low income\n",
    "    0.1 * (users_df['num_products'] == 1) +  # Single product\n",
    "    0.05 * (users_df['support_tickets'] > 2) +  # Many support tickets\n",
    "    0.1 * (users_df['account_tenure_months'] < 6)  # New accounts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some transaction-based features for churn\n",
    "user_tx_summary = transactions_df.groupby('user_id').agg({\n",
    "    'transaction_frequency': 'mean',\n",
    "    'days_since_last_transaction': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "users_df = users_df.merge(user_tx_summary, on='user_id', how='left')\n",
    "users_df['transaction_frequency'] = users_df['transaction_frequency'].fillna(0)\n",
    "users_df['days_since_last_transaction'] = users_df['days_since_last_transaction'].fillna(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad962477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust churn probability based on transaction patterns\n",
    "churn_probability += 0.2 * (users_df['transaction_frequency'] < 0.5)  # Low frequency\n",
    "churn_probability += 0.15 * (users_df['days_since_last_transaction'] > 90)  # Long absence\n",
    "\n",
    "users_df['churn_risk'] = np.random.binomial(1, churn_probability.clip(0, 0.8), n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop helper columns used for churn generation\n",
    "users_df = users_df.drop(['transaction_frequency', 'days_since_last_transaction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0294ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "users_df.to_csv('datasets/users.csv', index=False)\n",
    "transactions_df.to_csv('datasets/transactions.csv', index=False)\n",
    "\n",
    "print(\"üìä Synthetic datasets created successfully!\")\n",
    "print(f\"üë• Users dataset: {len(users_df):,} records, {len(users_df.columns)} columns\")\n",
    "print(f\"üí≥ Transactions dataset: {len(transactions_df):,} records, {len(transactions_df.columns)} columns\")\n",
    "print(f\"üéØ Churn rate: {users_df['churn_risk'].mean():.1%}\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nüìã Users dataset preview:\")\n",
    "print(users_df.head())\n",
    "print(\"\\nüìã Transactions dataset preview:\")  \n",
    "print(transactions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9768d51",
   "metadata": {},
   "source": [
    "## Section 3: Configuration\n",
    "\n",
    "### Configuring the Model\n",
    "\n",
    "NexusFlow is configured using a single YAML file that specifies:\n",
    "- **Project settings** (name, primary key, target variable)\n",
    "- **Dataset configurations** (transformer types, complexity levels)\n",
    "- **Architecture settings** (embedding dimensions, refinement iterations)\n",
    "- **Advanced features** (MoE, FlashAttention, preprocessing)\n",
    "- **Training parameters** (batch size, epochs, optimization)\n",
    "\n",
    "Let's create a sophisticated configuration that showcases NexusFlow's advanced capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configs/config.yaml\n",
    "project_name: \"nexusflow_churn_prediction\"\n",
    "\n",
    "target:\n",
    "  target_column: \"churn_risk\"\n",
    "  target_table: \"users.csv\"\n",
    "\n",
    "# Enhanced dataset configurations with true relational support\n",
    "datasets:\n",
    "  - name: \"users.csv\"\n",
    "    transformer_type: \"ft_transformer\"\n",
    "    complexity: \"medium\"\n",
    "    context_weight: 1.0\n",
    "    primary_key: [\"user_id\"]  # Composite key support\n",
    "    categorical_columns: [\"region\", \"subscription_tier\"]\n",
    "    numerical_columns: [\"age\", \"income\", \"account_tenure_months\", \"num_products\", \"support_tickets\"]\n",
    "  \n",
    "  - name: \"transactions.csv\"\n",
    "    transformer_type: \"standard\"\n",
    "    complexity: \"medium\"\n",
    "    context_weight: 0.8\n",
    "    primary_key: [\"transaction_id\"]\n",
    "    foreign_keys:\n",
    "      - columns: [\"user_id\"]  # FK to users table\n",
    "        references_table: \"users.csv\"\n",
    "        references_columns: [\"user_id\"]\n",
    "    categorical_columns: [\"payment_method\", \"transaction_category\"]\n",
    "    numerical_columns: [\"transaction_amount\", \"transaction_frequency\", \"days_since_last_transaction\", \"merchant_rating\"]\n",
    "\n",
    "# Advanced architecture configuration\n",
    "architecture:\n",
    "  global_embed_dim: 128\n",
    "  refinement_iterations: 4\n",
    "  use_moe: true\n",
    "  num_experts: 6\n",
    "  use_flash_attn: true\n",
    "  top_k_contexts: 3\n",
    "\n",
    "# Training configuration\n",
    "training:\n",
    "  batch_size: 32\n",
    "  epochs: 5\n",
    "  optimizer:\n",
    "    name: \"adam\"\n",
    "    lr: 0.001\n",
    "    weight_decay: 0.0001\n",
    "  \n",
    "  use_advanced_preprocessing: true\n",
    "  auto_detect_types: true\n",
    "  early_stopping: true\n",
    "  patience: 5\n",
    "  gradient_clipping: 1.0\n",
    "  \n",
    "  split_config:\n",
    "    test_size: 0.2\n",
    "    validation_size: 0.2\n",
    "    randomize: true\n",
    "\n",
    "# MLOps configuration\n",
    "mlops:\n",
    "  logging_provider: \"stdout\"\n",
    "  experiment_name: \"churn_prediction_demo\"\n",
    "  log_attention_patterns: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Configuration file created successfully!\")\n",
    "print(\"üìù Key configuration highlights:\")\n",
    "print(\"   üîÑ FT-Transformer for users table (advanced tabular processing)\")  \n",
    "print(\"   ‚ö° Standard transformer with FlashAttention for transactions\")\n",
    "print(\"   üîÄ Mixture of Experts enabled (6 experts)\")\n",
    "print(\"   üß† Advanced preprocessing with auto-detection\")\n",
    "print(\"   üéØ Binary classification: churn risk prediction\")\n",
    "print(\"   üìä 4 refinement iterations for cross-table learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8130488",
   "metadata": {},
   "source": [
    "## Section 4: Training the Model\n",
    "\n",
    "### Let's Train!\n",
    "\n",
    "With our data and configuration ready, training a sophisticated multi-table model is remarkably simple. NexusFlow handles all the complexity behind the scenes:\n",
    "\n",
    "- **Automatic data loading and preprocessing**\n",
    "- **Cross-table relationship modeling**\n",
    "- **Advanced transformer architectures**\n",
    "- **Mixture of Experts routing**\n",
    "- **FlashAttention optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0054b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and initialize trainer\n",
    "config = load_config_from_file('configs/config.yaml')\n",
    "trainer = Trainer(config, work_dir='.')\n",
    "\n",
    "print(\"üéØ Starting NexusFlow training with advanced features...\")\n",
    "print(f\"üìä Model architecture: {config.architecture.num_experts} MoE experts, {config.architecture.global_embed_dim}D embeddings\")\n",
    "print(f\"‚ö° Optimizations: FlashAttention={config.architecture.use_flash_attn}, MoE={config.architecture.use_moe}\")\n",
    "print(f\"üîÑ Cross-table refinement: {config.architecture.refinement_iterations} iterations\")\n",
    "\n",
    "# Run training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nüéâ Training completed successfully!\")\n",
    "print(\"üìÅ Artifacts created:\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction.nxf (complete model artifact)\")\n",
    "print(\"   üíæ best_model.pt (PyTorch checkpoint)\")\n",
    "print(\"   üìä results/training_history.json (training metrics)\")\n",
    "print(\"   üîß preprocessing/ (preprocessing artifacts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81035c",
   "metadata": {},
   "source": [
    "## Section 5: Using the Trained Model\n",
    "\n",
    "### Model Loading and Inference\n",
    "\n",
    "Our training produced a complete `.nxf` model artifact that contains:\n",
    "- **The trained NexusFormer model**\n",
    "- **All preprocessing transformations**\n",
    "- **Model configuration and metadata**\n",
    "- **Performance metrics**\n",
    "\n",
    "Let's load this artifact and make predictions on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model artifact\n",
    "model = load_model('nexusflow_churn_prediction.nxf')\n",
    "\n",
    "print(\"üì¶ Model artifact loaded successfully!\")\n",
    "print(\"\\nüìã Model Summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "# Get detailed model parameters\n",
    "params = model.get_params()\n",
    "print(f\"\\nüîç Model Details:\")\n",
    "print(f\"   Total Parameters: {params['total_parameters']:,}\")\n",
    "print(f\"   Architecture: {params['model_class']} with {params['architecture']['num_encoders']} encoders\")\n",
    "print(f\"   Training Accuracy: {params['training_info']['best_val_metric']:.4f}\")\n",
    "print(f\"   Preprocessing: {'Enabled' if params.get('phase_2_features', {}).get('advanced_preprocessing', False) else 'Basic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271983cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new sample data for inference (simulating new users and their transactions)\n",
    "np.random.seed(123)  # Different seed for new data\n",
    "\n",
    "# New users data\n",
    "new_users = pd.DataFrame({\n",
    "    'user_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'age': [28, 45, 36, 52, 31],\n",
    "    'income': [45000, 85000, 62000, 120000, 38000],\n",
    "    'account_tenure_months': [3, 24, 12, 48, 6],\n",
    "    'num_products': [1, 3, 2, 5, 1],\n",
    "    'region': ['North', 'South', 'East', 'West', 'Central'],\n",
    "    'subscription_tier': ['Basic', 'Premium', 'Basic', 'Enterprise', 'Basic'],\n",
    "    'support_tickets': [2, 0, 1, 0, 3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New transactions data\n",
    "new_transactions = pd.DataFrame({\n",
    "    'user_id': [1001, 1002, 1003, 1004, 1004, 1005],\n",
    "    'transaction_amount': [45.0, 89.99, 89.99, 23.23, 1250.00, 15.99],\n",
    "    'transaction_frequency': [0.5, 2.4, 1.8, 1.2, 3.5, 3.5],\n",
    "    'days_since_last_transaction': [45, 7, 1, 5, 78, 89],\n",
    "    'payment_method': ['Credit', 'Digital', 'Digital', 'Credit', 'Credit', 'Cash'],\n",
    "    'transaction_category': ['Retail', 'Retail', 'Retail', 'Services', 'Services', 'Other'],\n",
    "    'merchant_rating': [4.2, 3.8, 4.5, 4.9, 3.2, 3.2]\n",
    "})\n",
    "\n",
    "print(\"üë§ New sample data created:\")\n",
    "print(new_users[['user_id', 'age', 'income', 'subscription_tier', 'support_tickets']])\n",
    "print(f\"\\nüí≥ Associated transactions: {len(new_transactions)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Users\")\n",
    "print(new_users)\n",
    "print(\"New Transactions\")\n",
    "print(new_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the multi-table format\n",
    "prediction_data = {\n",
    "    'users.csv': new_users,\n",
    "    'transactions.csv': new_transactions\n",
    "}\n",
    "\n",
    "# Get predictions (churn probabilities)\n",
    "predictions = model.predict(prediction_data)\n",
    "\n",
    "print(\"üéØ Churn Risk Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results_df = new_users[['user_id', 'age', 'income', 'subscription_tier']].copy()\n",
    "results_df['churn_probability'] = predictions\n",
    "results_df['risk_level'] = results_df['churn_probability'].apply(\n",
    "    lambda x: 'High' if x > 0.7 else ('Medium' if x > 0.4 else 'Low')\n",
    ")\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"User {row['user_id']:4d}: {row['churn_probability']:.3f} ({row['risk_level']:6s}) - \"\n",
    "          f\"Age {row['age']}, {row['subscription_tier']:10s}, ${row['income']:,}\")\n",
    "\n",
    "print(f\"\\nüìä Risk Distribution:\")\n",
    "print(f\"   High Risk (>70%):   {sum(results_df['risk_level'] == 'High')} users\")\n",
    "print(f\"   Medium Risk (40-70%): {sum(results_df['risk_level'] == 'Medium')} users\") \n",
    "print(f\"   Low Risk (<40%):    {sum(results_df['risk_level'] == 'Low')} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27357c0f",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "NexusFlow includes state-of-the-art model optimization techniques to reduce model size and improve inference speed without sacrificing accuracy. Let's demonstrate both quantization and pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original trained model\n",
    "original_artifact = load_model('nexusflow_churn_prediction.nxf')\n",
    "\n",
    "print(\"üîß Model Optimization Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "params_info = original_artifact.get_params()\n",
    "original_params = params_info['total_parameters']\n",
    "original_size_mb = original_params * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"üìä Original Model:\")\n",
    "print(f\"   Parameters: {original_params:,}\")\n",
    "print(f\"   Size: {original_size_mb:.2f} MB\")\n",
    "\n",
    "# Extract the actual PyTorch model for optimization\n",
    "actual_model = extract_pytorch_model(original_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quantization\n",
    "print(\"\\n‚ö° Applying Dynamic Quantization...\")\n",
    "quantized_pytorch_model, quant_metadata = optimize_model(actual_model, method='quantization')\n",
    "\n",
    "print(f\"\\nüìà Quantization Results:\")\n",
    "print(f\"   Method: {quant_metadata['method']}\")\n",
    "print(f\"   Size Reduction: {quant_metadata['size_reduction']:.1%}\")\n",
    "print(f\"   Parameter Reduction: {quant_metadata['parameter_reduction']:.1%}\")\n",
    "print(f\"   Original Size: {quant_metadata['original_size_mb']:.2f} MB\")\n",
    "print(f\"   Optimized Size: {quant_metadata['optimized_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a039521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pruning\n",
    "print(\"\\n‚úÇÔ∏è  Applying Global Unstructured Pruning...\")\n",
    "pruned_pytorch_model, prune_metadata = optimize_model(actual_model, method='pruning', amount=0.3)\n",
    "\n",
    "print(f\"\\nüìà Pruning Results:\")\n",
    "print(f\"   Method: {prune_metadata['method']}\")\n",
    "print(f\"   Pruning Amount: 30%\")\n",
    "print(f\"   Actual Size Reduction: {prune_metadata['size_reduction']:.1%}\")\n",
    "print(f\"   Parameter Reduction: {prune_metadata['parameter_reduction']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized artifacts properly\n",
    "print(\"\\nüíæ Saving Optimized Models...\")\n",
    "\n",
    "# Create quantized artifact\n",
    "quantized_artifact = create_optimized_model(original_artifact, quantized_pytorch_model, quant_metadata)\n",
    "NexusFlowModel(quantized_pytorch_model, quantized_artifact.meta).save('nexusflow_churn_prediction_quantized.nxf')\n",
    "\n",
    "# Create pruned artifact  \n",
    "pruned_artifact = create_optimized_model(original_artifact, pruned_pytorch_model, prune_metadata)\n",
    "NexusFlowModel(pruned_pytorch_model, pruned_artifact.meta).save('nexusflow_churn_prediction_pruned.nxf')\n",
    "\n",
    "print(\"‚úÖ Optimized model artifacts saved:\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction_quantized.nxf\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction_pruned.nxf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a65d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all model sizes\n",
    "import os\n",
    "original_size = os.path.getsize('nexusflow_churn_prediction.nxf') / (1024 * 1024)\n",
    "quantized_size = os.path.getsize('nexusflow_churn_prediction_quantized.nxf') / (1024 * 1024)  \n",
    "pruned_size = os.path.getsize('nexusflow_churn_prediction_pruned.nxf') / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìä Final Model Comparison:\")\n",
    "print(f\"   Original:  {original_size:.2f} MB (100%)\")\n",
    "print(f\"   Quantized: {quantized_size:.2f} MB ({quantized_size/original_size*100:.1f}%)\")\n",
    "print(f\"   Pruned:    {pruned_size:.2f} MB ({pruned_size/original_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a35b58",
   "metadata": {},
   "source": [
    "## Section 6: Advanced Features Deep Dive\n",
    "\n",
    "### Cross-Contextual Attention Analysis\n",
    "\n",
    "One of NexusFormer's key innovations is its cross-contextual attention mechanism, which learns relationships between different tables. Let's examine how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f80978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture and attention patterns\n",
    "print(\"üß† NexusFormer Architecture Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model = original_artifact\n",
    "\n",
    "architecture_info = model.visualize_flow()\n",
    "\n",
    "print(f\"üèóÔ∏è  Model Architecture:\")\n",
    "print(f\"   Architecture Type: {architecture_info['architecture']}\")\n",
    "print(f\"   Number of Contextual Encoders: {architecture_info['num_encoders']}\")\n",
    "print(f\"   Input Dimensions: {architecture_info['input_dims']}\")\n",
    "print(f\"   Refinement Iterations: {architecture_info['refinement_iterations']}\")\n",
    "\n",
    "print(f\"\\nüîÑ Learning Process:\")\n",
    "print(f\"   1. Each table (users, transactions) is processed by specialized encoders\")\n",
    "print(f\"   2. Cross-contextual attention learns relationships between tables\")\n",
    "print(f\"   3. {architecture_info['refinement_iterations']} refinement iterations enhance representations\")\n",
    "print(f\"   4. Final fusion layer combines all contextual information\")\n",
    "print(f\"   5. Prediction head outputs churn probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910891a",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23063af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the preprocessing pipeline that was automatically applied\n",
    "if hasattr(model, 'get_params'):\n",
    "    params = model.get_params()\n",
    "    if 'phase_2_features' in params and params['phase_2_features']['advanced_preprocessing']:\n",
    "        print(\"\\nüîß Advanced Preprocessing Pipeline Applied:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        preprocessing_datasets = params['phase_2_features']['preprocessor_datasets']\n",
    "        print(f\"‚úÖ Datasets processed: {preprocessing_datasets}\")\n",
    "        print(\"üîç Automatic feature type detection enabled\")\n",
    "        print(\"üìä Advanced categorical encoding applied\")\n",
    "        print(\"üìà Numerical feature normalization applied\")\n",
    "        print(\"üö´ Missing value imputation handled\")\n",
    "        \n",
    "        print(f\"\\nüìã Feature Processing Summary:\")\n",
    "        for dataset in preprocessing_datasets:\n",
    "            print(f\"   {dataset.title()} table: Categorical & numerical features automatically detected\")\n",
    "    else:\n",
    "        print(\"\\nüí° Basic preprocessing was used (fillna-based)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ab4ae",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation & Insights\n",
    "\n",
    "### Comprehensive Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history and analyze model performance\n",
    "with open('results/training_history.json', 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "history = training_data['training_history']\n",
    "best_epoch = training_data['best_epoch']\n",
    "best_metric = training_data['best_metric']\n",
    "\n",
    "print(\"üìà Training Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üèÜ Best Model: Epoch {best_epoch} with validation loss {best_metric:.6f}\")\n",
    "print(f\"üìä Total Epochs: {len(history)}\")\n",
    "print(f\"‚è∞ Early Stopping: {'Yes' if training_data.get('early_stopped', False) else 'No'}\")\n",
    "print(f\"üîß Advanced Preprocessing: {'Yes' if training_data.get('preprocessing_enabled', False) else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "epochs = [h['epoch'] for h in history]\n",
    "train_losses = [h['train_loss'] for h in history]\n",
    "val_losses = [h.get('val_loss', None) for h in history if h.get('val_loss') is not None]\n",
    "\n",
    "print(f\"\\nüìâ Training Progress:\")\n",
    "print(f\"   Initial Training Loss: {train_losses[0]:.6f}\")\n",
    "print(f\"   Final Training Loss: {train_losses[-1]:.6f}\")\n",
    "if val_losses:\n",
    "    print(f\"   Initial Validation Loss: {val_losses[0]:.6f}\")\n",
    "    print(f\"   Best Validation Loss: {min(val_losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "eval_results = model.evaluate()\n",
    "if eval_results:\n",
    "    print(f\"\\nüéØ Model Performance Metrics:\")\n",
    "    for metric, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"   {metric.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fedc51",
   "metadata": {},
   "source": [
    "## Section 8: Conclusion & Next Steps\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the NexusFlow tutorial and experienced the power of advanced multi-table machine learning! Here's what you accomplished:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b7265",
   "metadata": {},
   "source": [
    "**‚úÖ What You Built:**\n",
    "- **Multi-table ML model** using users and transactions data\n",
    "- **Advanced transformer architectures** (FT-Transformer + Standard)\n",
    "- **Mixture of Experts** for handling complex patterns\n",
    "- **FlashAttention optimization** for efficient processing\n",
    "- **Cross-contextual learning** between related tables\n",
    "- **Advanced preprocessing** with automatic feature detection\n",
    "- **Model optimization** with quantization and pruning\n",
    "\n",
    "**üöÄ Key Takeaways:**\n",
    "1. **Multi-table learning** can capture complex relationships that single-table models miss\n",
    "2. **Advanced architectures** like MoE and FlashAttention provide both performance and efficiency\n",
    "3. **Automatic preprocessing** reduces manual feature engineering overhead\n",
    "4. **Model optimization** can dramatically reduce deployment costs\n",
    "5. **NexusFlow's unified API** makes complex ML accessible\n",
    "\n",
    "### Next Steps & Advanced Usage:\n",
    "\n",
    "1. üìä Try with your own multi-table datasets\n",
    "2. üîß Experiment with different transformer types (TabNet, etc.)\n",
    "3. üéõÔ∏è  Tune hyperparameters for your specific domain\n",
    "4. üîÄ Explore more MoE configurations\n",
    "5. üìà Scale to larger datasets and more tables\n",
    "6. üåê Deploy optimized models to production\n",
    "7. üìö Check the NexusFlow documentation for advanced features\n",
    "\n",
    "Pro Tips\n",
    "1. Use 'ft_transformer' for tables with mixed categorical/numerical features\n",
    "2. Enable MoE for datasets with complex, diverse patterns\n",
    "3. Apply model optimization before production deployment\n",
    "4. Experiment with refinement_iterations for better cross-table learning\n",
    "5. Monitor attention patterns for model interpretability\n",
    "\n",
    "Your artifacts are ready for production:\n",
    "- nexusflow_churn_prediction.nxf (original model)\n",
    "- nexusflow_churn_prediction_quantized.nxf (optimized for speed)\n",
    "-  nexusflow_churn_prediction_pruned.nxf (optimized for size)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation:** Check the NexusFlow docs for comprehensive guides\n",
    "- **GitHub:** Explore the source code and contribute  \n",
    "- **Community:** Join discussions and share your experiences\n",
    "- **Examples:** Browse more advanced use cases and tutorials\n",
    "\n",
    "**Happy modeling with NexusFlow!** üöÄ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abc696",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
